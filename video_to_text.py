from transformers import pipeline
from PIL import Image
import cv2
import numpy as np
import torch
from googletrans import Translator

# T·∫°o pipeline m√¥ t·∫£ h√¨nh ·∫£nh b·∫±ng m√¥ h√¨nh BLIP
captioner = pipeline("image-to-text", model="Salesforce/blip-image-captioning-large")

# T·∫°o ƒë·ªëi t∆∞·ª£ng d·ªãch t·ª± ƒë·ªông sang ti·∫øng Vi·ªát
translator = Translator()

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn video b·∫°n mu·ªën x·ª≠ l√Ω
video_path = "public/mixkit-chopping-fruit-for-a-salad-43913-hd-ready.mp4"  # ƒë·ªïi th√†nh t√™n file video c·ªßa b·∫°n

# M·ªü video b·∫±ng OpenCV
cap = cv2.VideoCapture(video_path)

# C·∫•u h√¨nh: m·ªói X gi√¢y l·∫•y 1 khung h√¨nh
frame_rate = 1  # l·∫•y 1 khung h√¨nh m·ªói gi√¢y
frame_id = 0
fps = cap.get(cv2.CAP_PROP_FPS)
frame_interval = int(fps * frame_rate)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break  # n·∫øu kh√¥ng c√≤n frame th√¨ tho√°t

    if frame_id % frame_interval == 0:
        # Chuy·ªÉn t·ª´ BGR (OpenCV) sang RGB (chu·∫©n c·ªßa PIL)
        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # M√¥ t·∫£ h√¨nh ·∫£nh b·∫±ng ti·∫øng Anh
        result = captioner(image)
        english_caption = result[0]['generated_text']

        # D·ªãch sang ti·∫øng Vi·ªát
        translated = translator.translate(english_caption, src='en', dest='vi').text

        # In k·∫øt qu·∫£ ra m√†n h√¨nh
        print(f"üñºÔ∏è Khung h√¨nh {frame_id}: {translated} (EN: {english_caption})")

    frame_id += 1

cap.release()
