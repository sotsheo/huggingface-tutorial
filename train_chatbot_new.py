import pandas as pd
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TextDataset,
    DataCollatorForLanguageModeling,
    pipeline
)
import torch
import os

# === 1. Load CSV v√† chu·∫©n h√≥a d·ªØ li·ªáu ===
CSV_FILE = "public/products_10000.csv"

df = pd.read_csv(CSV_FILE)
df = df.dropna(subset=["name", "features", "description"])
df["input"] = "Th√¥ng tin s·∫£n ph·∫©m: " + df["name"] + " | " + df["features"]
df["output"] = df["description"]

# === 2. Ghi d·ªØ li·ªáu hu·∫•n luy·ªán v√†o file text ===
TRAIN_FILE = "train_data.txt"
with open(TRAIN_FILE, "w", encoding="utf-8") as f:
    for i, o in zip(df["input"], df["output"]):
        f.write(f"<s>Kh√°ch h·ªèi: {i}\nChatbot: {o}</s>\n")

# === 3. Load tokenizer v√† model ===
MODEL_NAME = "VietAI/gpt2-small"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# === 4. Load t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán ===
def load_dataset(file_path, tokenizer):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=128,
    )

train_dataset = load_dataset(TRAIN_FILE, tokenizer)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# === 5. C·∫•u h√¨nh v√† hu·∫•n luy·ªán ===
training_args = TrainingArguments(
    output_dir="./chatbot_model",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=1,
    fp16=False,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)

print("=== üîß Hu·∫•n luy·ªán m√¥ h√¨nh... ===")
trainer.train()
trainer.save_model("./chatbot_model")
tokenizer.save_pretrained("./chatbot_model")
print("‚úÖ Hu·∫•n luy·ªán xong!")

# === 6. H√†m t∆∞∆°ng t√°c v·ªõi chatbot ===
print("ü§ñ ƒêang n·∫°p m√¥ h√¨nh hu·∫•n luy·ªán xong ƒë·ªÉ tr√≤ chuy·ªán...")
chatbot = pipeline("text-generation", model="./chatbot_model", tokenizer=tokenizer)

def ask_chatbot(question):
    input_text = f"<s>Kh√°ch h·ªèi: {question}\nChatbot:"
    response = chatbot(input_text, max_length=150, do_sample=True, top_k=50)[0]["generated_text"]
    try:
        answer = response.split("Chatbot:")[1].strip().split("</s>")[0].strip()
        return answer
    except:
        return "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu r√µ c√¢u h·ªèi."

# === 7. Chat th·ª≠ ===
print("\n=== üí¨ B·∫Øt ƒë·∫ßu h·ªèi chatbot ===")
while True:
    q = input("B·∫°n: ")
    if q.lower() in ["exit", "quit", "bye"]:
        print("Chatbot: T·∫°m bi·ªát b·∫°n nh√©!")
        break
    print("Chatbot:", ask_chatbot(q))
